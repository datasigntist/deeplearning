{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pySpark_Codes_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datasigntist/deeplearning/blob/master/pySpark_Codes_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZJSYg2FvsDl",
        "colab_type": "text"
      },
      "source": [
        "# **Experiments with Spark**\n",
        "\n",
        "Author : Vishwanathan Raman\n",
        "\n",
        "EmailId : datasigntist@gmail.com\n",
        "\n",
        "Description : This notebook covers basic to intermediate concepts through practical experimentation on pySpark\n",
        "\n",
        "Reference Links:\n",
        "\n",
        "*   Introduction to Spark 1 : https://youtu.be/TuGn3e1EgXM\n",
        "*   Introduction to Spark 2 : https://youtu.be/JruCKuWHKpk\n",
        "*   Introduction to Spark 3 : https://youtu.be/c9jd4yZGyT8\n",
        "*   Introduction to RDD 1   : https://youtu.be/M7UuKHYecXQ\n",
        "*   Introduction to RDD 2   : https://youtu.be/qLGUPdSvAVg\n",
        "*   Introduction to RDD 3   : https://youtu.be/9NBP-FiHrQg\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDKi0NXTwTUK",
        "colab_type": "text"
      },
      "source": [
        "## **Spark Installation in Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a00WIwTYjZU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecTfHk4Hjd0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h1g1SeZx6gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agWbG96dx7Lx",
        "colab_type": "text"
      },
      "source": [
        "## **Loading text file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G70FqR_cj2jP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = spark.read.text(\"README.md\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygsIn-vgHeMC",
        "colab_type": "text"
      },
      "source": [
        "In Apache Spark, a DataFrame is a distributed collection of rows under named columns. In simple terms, it is same as a table in relational database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu8kBXDOz02a",
        "colab_type": "code",
        "outputId": "b37a2a70-fbb5-4e6c-9c17-0b92b49e3444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(type(data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnDzKTV_HGNy",
        "colab_type": "text"
      },
      "source": [
        "Converting the DataFrame to a RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69BcLWk2_7h7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_rdd = data.rdd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OztpTjVAEve",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65b0db82-7771-4ccb-912a-eb8f0c8a380f"
      },
      "source": [
        "print(type(data_rdd))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmQrgFVEAQyt",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Analysis of the dataframe through actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTqcXqlZlYul",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bab9c202-e011-4bd8-f044-45dd101c10bb"
      },
      "source": [
        "print(\"The number of lines in file README.md \",data.count())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of lines in file README.md  103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRQX6wXgAZ4F",
        "colab_type": "text"
      },
      "source": [
        "Retrieve the first row using the action first()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu0hAO8D_YTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94c7646d-5365-4430-9d3d-1f35e9d037f6"
      },
      "source": [
        "data.first()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(value='# Apache Spark')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0e-sFI-Ae-E",
        "colab_type": "text"
      },
      "source": [
        "Retrieve all the rows through collect() but limiting the retrieval to first 10 rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-01nARTAKus",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "249b29bc-cea5-4364-e49f-ba8404532c77"
      },
      "source": [
        "data.collect()[0:10]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(value='# Apache Spark'),\n",
              " Row(value=''),\n",
              " Row(value='Spark is a fast and general cluster computing system for Big Data. It provides'),\n",
              " Row(value='high-level APIs in Scala, Java, Python, and R, and an optimized engine that'),\n",
              " Row(value='supports general computation graphs for data analysis. It also supports a'),\n",
              " Row(value='rich set of higher-level tools including Spark SQL for SQL and DataFrames,'),\n",
              " Row(value='MLlib for machine learning, GraphX for graph processing,'),\n",
              " Row(value='and Spark Streaming for stream processing.'),\n",
              " Row(value=''),\n",
              " Row(value='<http://spark.apache.org/>')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FjcPGTiGvPj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4548907d-4855-4c4d-cbb8-9cc5f26a6714"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, value: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c36FNQDNDY4_",
        "colab_type": "text"
      },
      "source": [
        "### Applying transformations on the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ogvM1PmGKIr",
        "colab_type": "text"
      },
      "source": [
        "Filter the data having Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUbGY0JVAiFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linesWithSpark = data.filter(data.value.contains(\"Spark\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW0gvHcUHqwc",
        "colab_type": "text"
      },
      "source": [
        "Creating a new RDD based on the existing rdd through transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YYeYc_WGh6y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab0e8cbe-a50b-49ec-d3aa-f7a1b978962a"
      },
      "source": [
        "print(type(linesWithSpark))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aumWtmChHo6z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf01646d-cea9-4eee-87db-be0cd1714f9d"
      },
      "source": [
        "print(\"The number of lines with Spark is \",linesWithSpark.count())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of lines with Spark is  20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyyUmxn9JVhS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "fb1a89b2-f0a1-4850-a649-07e72bfaa28a"
      },
      "source": [
        "linesWithSpark.collect()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(value='# Apache Spark'),\n",
              " Row(value='Spark is a fast and general cluster computing system for Big Data. It provides'),\n",
              " Row(value='rich set of higher-level tools including Spark SQL for SQL and DataFrames,'),\n",
              " Row(value='and Spark Streaming for stream processing.'),\n",
              " Row(value='You can find the latest Spark documentation, including a programming'),\n",
              " Row(value='## Building Spark'),\n",
              " Row(value='Spark is built using [Apache Maven](http://maven.apache.org/).'),\n",
              " Row(value='To build Spark and its example programs, run:'),\n",
              " Row(value='You can build Spark using more than one thread by using the -T option with Maven, see [\"Parallel builds in Maven 3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).'),\n",
              " Row(value='[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).'),\n",
              " Row(value='For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](http://spark.apache.org/developer-tools.html).'),\n",
              " Row(value='The easiest way to start using Spark is through the Scala shell:'),\n",
              " Row(value='Spark also comes with several sample programs in the `examples` directory.'),\n",
              " Row(value='    ./bin/run-example SparkPi'),\n",
              " Row(value='    MASTER=spark://host:7077 ./bin/run-example SparkPi'),\n",
              " Row(value='Testing first requires [building Spark](#building-spark). Once Spark is built, tests'),\n",
              " Row(value='Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported'),\n",
              " Row(value='Hadoop, you must build Spark against the same version that your cluster runs.'),\n",
              " Row(value='in the online documentation for an overview on how to configure Spark.'),\n",
              " Row(value='Please review the [Contribution to Spark guide](http://spark.apache.org/contributing.html)')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVR9UxN2M5iK",
        "colab_type": "text"
      },
      "source": [
        "Applying transformations on the RDD which was created in the earlier step. This is different from the dataframe object. Each line in the rdd has a value which is being scanned for the word Spark. The result is the same as in the previous step except that we are scanning the RDD instead of the dataframe object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkrabKPzKPwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linesWithSpark_rdd = data_rdd.filter(lambda line: \"Spark\" in line.value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_LNT_u-MiIY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc2f4bfc-2dab-4fcd-a78f-c8765584e708"
      },
      "source": [
        "print(type(linesWithSpark_rdd))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pyspark.rdd.PipelinedRDD'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3dUim7yNk6h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89a22e82-9c7c-4e83-a065-8c4e74e5c6e4"
      },
      "source": [
        "print(\"The number of lines having Spark \",linesWithSpark_rdd.count())"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of lines having Spark  20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsU2blk6NqUy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "51179e92-471f-44e4-c014-5d0436debecb"
      },
      "source": [
        "linesWithSpark_rdd.collect()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(value='# Apache Spark'),\n",
              " Row(value='Spark is a fast and general cluster computing system for Big Data. It provides'),\n",
              " Row(value='rich set of higher-level tools including Spark SQL for SQL and DataFrames,'),\n",
              " Row(value='and Spark Streaming for stream processing.'),\n",
              " Row(value='You can find the latest Spark documentation, including a programming'),\n",
              " Row(value='## Building Spark'),\n",
              " Row(value='Spark is built using [Apache Maven](http://maven.apache.org/).'),\n",
              " Row(value='To build Spark and its example programs, run:'),\n",
              " Row(value='You can build Spark using more than one thread by using the -T option with Maven, see [\"Parallel builds in Maven 3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).'),\n",
              " Row(value='[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).'),\n",
              " Row(value='For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](http://spark.apache.org/developer-tools.html).'),\n",
              " Row(value='The easiest way to start using Spark is through the Scala shell:'),\n",
              " Row(value='Spark also comes with several sample programs in the `examples` directory.'),\n",
              " Row(value='    ./bin/run-example SparkPi'),\n",
              " Row(value='    MASTER=spark://host:7077 ./bin/run-example SparkPi'),\n",
              " Row(value='Testing first requires [building Spark](#building-spark). Once Spark is built, tests'),\n",
              " Row(value='Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported'),\n",
              " Row(value='Hadoop, you must build Spark against the same version that your cluster runs.'),\n",
              " Row(value='in the online documentation for an overview on how to configure Spark.'),\n",
              " Row(value='Please review the [Contribution to Spark guide](http://spark.apache.org/contributing.html)')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNkYKZhfOCo7",
        "colab_type": "text"
      },
      "source": [
        "Adding additional transformation of splitting the line into individual words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qGNANX9Ns8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordsInlinesWithSpark = linesWithSpark_rdd.flatMap(lambda line: line.value.split(\" \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FPfyLOlOXf1",
        "colab_type": "text"
      },
      "source": [
        "The following is a representation of the different operations in the RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNSYct6JORHE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae14aefb-7698-4eaf-8292-6f178adcd041"
      },
      "source": [
        "print(type(wordsInlinesWithSpark))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pyspark.rdd.PipelinedRDD'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voCfx65dOOY0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1ccc4343-e2e5-4283-cb69-93a96699575b"
      },
      "source": [
        "wordsInlinesWithSpark.toDebugString()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'(1) PythonRDD[58] at RDD at PythonRDD.scala:53 []\\n |  MapPartitionsRDD[9] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[8] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  FileScanRDD[6] at javaToPython at NativeMethodAccessorImpl.java:0 []'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPq_zOr8PCVi",
        "colab_type": "text"
      },
      "source": [
        "Listing the first 20 words in the RDD wordsInlinesWithSpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF2fth6tOuwE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "f657d709-0109-4515-c306-c3735557146c"
      },
      "source": [
        "wordsInlinesWithSpark.collect()[0:20]"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#',\n",
              " 'Apache',\n",
              " 'Spark',\n",
              " 'Spark',\n",
              " 'is',\n",
              " 'a',\n",
              " 'fast',\n",
              " 'and',\n",
              " 'general',\n",
              " 'cluster',\n",
              " 'computing',\n",
              " 'system',\n",
              " 'for',\n",
              " 'Big',\n",
              " 'Data.',\n",
              " 'It',\n",
              " 'provides',\n",
              " 'rich',\n",
              " 'set',\n",
              " 'of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is9qjreYPcB7",
        "colab_type": "text"
      },
      "source": [
        "Defining a function to convert all words to lowercase. Applying the function using the map operation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDbK3o2_O-ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to convert to lowercase\n",
        "def convertToLower(word):\n",
        "    word = word.lower()\n",
        "    return word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHraQGsdPRzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordsinlinesWithSpark_toLowerCase = wordsInlinesWithSpark.map(convertToLower)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGiRCN0wPaTV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "c88ac2ca-79c4-4360-df9b-d553d75e0244"
      },
      "source": [
        "wordsinlinesWithSpark_toLowerCase.collect()[0:20]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#',\n",
              " 'apache',\n",
              " 'spark',\n",
              " 'spark',\n",
              " 'is',\n",
              " 'a',\n",
              " 'fast',\n",
              " 'and',\n",
              " 'general',\n",
              " 'cluster',\n",
              " 'computing',\n",
              " 'system',\n",
              " 'for',\n",
              " 'big',\n",
              " 'data.',\n",
              " 'it',\n",
              " 'provides',\n",
              " 'rich',\n",
              " 'set',\n",
              " 'of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnrS9JDzPvLa",
        "colab_type": "text"
      },
      "source": [
        "Executing the take function, take function is similar to head in pandas. In take we can specify the number of retrievals which will be more efficient than collect and filter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CBMgs9RPl3R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "e1fefda1-2521-4897-abd2-8827dcc7f929"
      },
      "source": [
        "wordsinlinesWithSpark_toLowerCase.take(10)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#',\n",
              " 'apache',\n",
              " 'spark',\n",
              " 'spark',\n",
              " 'is',\n",
              " 'a',\n",
              " 'fast',\n",
              " 'and',\n",
              " 'general',\n",
              " 'cluster']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8VqhZUzP4Pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = ['#','[',']','is','for','a','and','.','']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOgnJ8YSRxab",
        "colab_type": "text"
      },
      "source": [
        "Removing all the stop words from the list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAyaidAkRixK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordsinlinesWithSpark_toLowerCase_afterStopWordRemoval = wordsinlinesWithSpark_toLowerCase.filter(lambda word: word not in stopwords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id1oObl5RvoG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "03866d4f-56d0-45c0-ce16-252f2d870257"
      },
      "source": [
        "wordsinlinesWithSpark_toLowerCase_afterStopWordRemoval.take(10)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apache',\n",
              " 'spark',\n",
              " 'spark',\n",
              " 'fast',\n",
              " 'general',\n",
              " 'cluster',\n",
              " 'computing',\n",
              " 'system',\n",
              " 'big',\n",
              " 'data.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyJVqXnWS3ob",
        "colab_type": "text"
      },
      "source": [
        "Creating a key value pair using the map function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehtv0sB1SvNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordDict = wordsinlinesWithSpark_toLowerCase_afterStopWordRemoval.map(lambda word: (word, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fUugt1WTzEa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "845083ec-b874-47e0-b50f-3eeb3600dee9"
      },
      "source": [
        "wordDict.take(10)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('apache', 1),\n",
              " ('spark', 1),\n",
              " ('spark', 1),\n",
              " ('fast', 1),\n",
              " ('general', 1),\n",
              " ('cluster', 1),\n",
              " ('computing', 1),\n",
              " ('system', 1),\n",
              " ('big', 1),\n",
              " ('data.', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTT9qHlIUrst",
        "colab_type": "text"
      },
      "source": [
        "Coding lambda function to generate word counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UysVhzPPUIxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordFreq = wordDict.reduceByKey(lambda a, b: a + b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUT05JU1UwcS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26fbb7b4-ab07-41a0-a832-4fe2aa860e11"
      },
      "source": [
        "wordFreq.take(5)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('apache', 1), ('spark', 16), ('fast', 1), ('general', 2), ('cluster', 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDLg6N04Wqgs",
        "colab_type": "text"
      },
      "source": [
        "Sorting through lambda function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4z5JAYkUyIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordFreqSorted = wordFreq.sortBy(lambda a: a[1],False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mo06h3XVwqy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "613ae2bc-10d3-4615-b87e-7dbde3c1431e"
      },
      "source": [
        "wordFreqSorted.take(10)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark', 16),\n",
              " ('the', 9),\n",
              " ('to', 6),\n",
              " ('using', 5),\n",
              " ('including', 3),\n",
              " ('you', 3),\n",
              " ('build', 3),\n",
              " ('in', 3),\n",
              " ('general', 2),\n",
              " ('cluster', 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClS23g5sW7_F",
        "colab_type": "text"
      },
      "source": [
        "Printing the entire sequence of transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAMLZpsRWbUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "35c74e33-21e3-4a20-f281-c35d07843a10"
      },
      "source": [
        "wordFreqSorted.toDebugString()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'(1) PythonRDD[92] at RDD at PythonRDD.scala:53 []\\n |  MapPartitionsRDD[89] at mapPartitions at PythonRDD.scala:133 []\\n |  ShuffledRDD[88] at partitionBy at NativeMethodAccessorImpl.java:0 []\\n +-(1) PairwiseRDD[87] at reduceByKey at <ipython-input-106-90e3bc73a175>:1 []\\n    |  PythonRDD[86] at reduceByKey at <ipython-input-106-90e3bc73a175>:1 []\\n    |  MapPartitionsRDD[9] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n    |  MapPartitionsRDD[8] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n    |  MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n    |  FileScanRDD[6] at javaToPython at NativeMethodAccessorImpl.java:0 []'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiObcrz3YTDD",
        "colab_type": "text"
      },
      "source": [
        "Convert RDD to a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fqoAubEYSUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordFreqSorteddf = wordFreqSorted.toDF([\"word\",\"freq\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtrgkBC0YYfb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af49a85b-0547-4ea9-828c-6c1951036b40"
      },
      "source": [
        "print(type(wordFreqSorteddf))"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1L8Rv9iYIXs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "474e8b64-5558-4e07-aa0f-eed31375eb54"
      },
      "source": [
        "wordFreqSorteddf.collect()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(word='spark', freq=16),\n",
              " Row(word='the', freq=9),\n",
              " Row(word='to', freq=6),\n",
              " Row(word='using', freq=5),\n",
              " Row(word='including', freq=3),\n",
              " Row(word='you', freq=3),\n",
              " Row(word='build', freq=3),\n",
              " Row(word='in', freq=3),\n",
              " Row(word='general', freq=2),\n",
              " Row(word='cluster', freq=2),\n",
              " Row(word='sql', freq=2),\n",
              " Row(word='can', freq=2),\n",
              " Row(word='with', freq=2),\n",
              " Row(word='see', freq=2),\n",
              " Row(word='on', freq=2),\n",
              " Row(word='an', freq=2),\n",
              " Row(word='./bin/run-example', freq=2),\n",
              " Row(word='sparkpi', freq=2),\n",
              " Row(word='apache', freq=1),\n",
              " Row(word='fast', freq=1),\n",
              " Row(word='computing', freq=1),\n",
              " Row(word='system', freq=1),\n",
              " Row(word='big', freq=1),\n",
              " Row(word='data.', freq=1),\n",
              " Row(word='it', freq=1),\n",
              " Row(word='provides', freq=1),\n",
              " Row(word='rich', freq=1),\n",
              " Row(word='set', freq=1),\n",
              " Row(word='of', freq=1),\n",
              " Row(word='higher-level', freq=1),\n",
              " Row(word='tools', freq=1),\n",
              " Row(word='dataframes,', freq=1),\n",
              " Row(word='streaming', freq=1),\n",
              " Row(word='stream', freq=1),\n",
              " Row(word='processing.', freq=1),\n",
              " Row(word='find', freq=1),\n",
              " Row(word='latest', freq=1),\n",
              " Row(word='documentation,', freq=1),\n",
              " Row(word='programming', freq=1),\n",
              " Row(word='##', freq=1),\n",
              " Row(word='building', freq=1),\n",
              " Row(word='built', freq=1),\n",
              " Row(word='[apache', freq=1),\n",
              " Row(word='maven](http://maven.apache.org/).', freq=1),\n",
              " Row(word='its', freq=1),\n",
              " Row(word='example', freq=1),\n",
              " Row(word='programs,', freq=1),\n",
              " Row(word='run:', freq=1),\n",
              " Row(word='more', freq=1),\n",
              " Row(word='than', freq=1),\n",
              " Row(word='one', freq=1),\n",
              " Row(word='thread', freq=1),\n",
              " Row(word='by', freq=1),\n",
              " Row(word='-t', freq=1),\n",
              " Row(word='option', freq=1),\n",
              " Row(word='maven,', freq=1),\n",
              " Row(word='[\"parallel', freq=1),\n",
              " Row(word='builds', freq=1),\n",
              " Row(word='maven', freq=1),\n",
              " Row(word='3\"](https://cwiki.apache.org/confluence/display/maven/parallel+builds+in+maven+3).', freq=1),\n",
              " Row(word='[\"building', freq=1),\n",
              " Row(word='spark\"](http://spark.apache.org/docs/latest/building-spark.html).', freq=1),\n",
              " Row(word='development', freq=1),\n",
              " Row(word='tips,', freq=1),\n",
              " Row(word='info', freq=1),\n",
              " Row(word='developing', freq=1),\n",
              " Row(word='ide,', freq=1),\n",
              " Row(word='[\"useful', freq=1),\n",
              " Row(word='developer', freq=1),\n",
              " Row(word='tools\"](http://spark.apache.org/developer-tools.html).', freq=1),\n",
              " Row(word='easiest', freq=1),\n",
              " Row(word='way', freq=1),\n",
              " Row(word='start', freq=1),\n",
              " Row(word='through', freq=1),\n",
              " Row(word='scala', freq=1),\n",
              " Row(word='shell:', freq=1),\n",
              " Row(word='also', freq=1),\n",
              " Row(word='comes', freq=1),\n",
              " Row(word='several', freq=1),\n",
              " Row(word='sample', freq=1),\n",
              " Row(word='programs', freq=1),\n",
              " Row(word='`examples`', freq=1),\n",
              " Row(word='directory.', freq=1),\n",
              " Row(word='master=spark://host:7077', freq=1),\n",
              " Row(word='testing', freq=1),\n",
              " Row(word='first', freq=1),\n",
              " Row(word='requires', freq=1),\n",
              " Row(word='[building', freq=1),\n",
              " Row(word='spark](#building-spark).', freq=1),\n",
              " Row(word='once', freq=1),\n",
              " Row(word='built,', freq=1),\n",
              " Row(word='tests', freq=1),\n",
              " Row(word='uses', freq=1),\n",
              " Row(word='hadoop', freq=1),\n",
              " Row(word='core', freq=1),\n",
              " Row(word='library', freq=1),\n",
              " Row(word='talk', freq=1),\n",
              " Row(word='hdfs', freq=1),\n",
              " Row(word='other', freq=1),\n",
              " Row(word='hadoop-supported', freq=1),\n",
              " Row(word='hadoop,', freq=1),\n",
              " Row(word='must', freq=1),\n",
              " Row(word='against', freq=1),\n",
              " Row(word='same', freq=1),\n",
              " Row(word='version', freq=1),\n",
              " Row(word='that', freq=1),\n",
              " Row(word='your', freq=1),\n",
              " Row(word='runs.', freq=1),\n",
              " Row(word='online', freq=1),\n",
              " Row(word='documentation', freq=1),\n",
              " Row(word='overview', freq=1),\n",
              " Row(word='how', freq=1),\n",
              " Row(word='configure', freq=1),\n",
              " Row(word='spark.', freq=1),\n",
              " Row(word='please', freq=1),\n",
              " Row(word='review', freq=1),\n",
              " Row(word='[contribution', freq=1),\n",
              " Row(word='guide](http://spark.apache.org/contributing.html)', freq=1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHrU-Oxba_sj",
        "colab_type": "text"
      },
      "source": [
        "Converting to a Pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyIs-T-lZfvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordFreqSorteddf_pandas = wordFreqSorteddf.toPandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9Fh4cs0ahnT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79d79255-81d6-43fe-a70f-ce5fa530f8e3"
      },
      "source": [
        "type(wordFreqSorteddf_pandas)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    }
  ]
}